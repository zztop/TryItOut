{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpopular Category Selection\n",
    "\n",
    "### Description\n",
    "#### Data Mininig :-\n",
    "   In this report we will be trying to solve two different aspect of Text Information, by mining Yelp dataset. These can be summarized as\n",
    "   1. Finding dissimilarities between different categories of Cuisines by converting vectorizing words of reviews and using that to find least correlated Cuisine categories\n",
    "   2. We also deviced a way to Rate various restaurants for given category and state. We used review counts to get the most popular and sentiments for highest rated to rank a given restaurant.\n",
    "Finally using the above derived information we prepare the final dataset to be consumed by the Web application.\n",
    "\n",
    "#### Try It Out -  Web Application :-\n",
    "   In the web app we give users to try out a Cuisine which they may never try. Essentially we ask the users for the Cuisines they like the most. Using the reviews for all the cuisine categories, we find the one which is `Least Correlated` to that particular category. We give users a choice of state they are from, and using the least correlated category provide the top three most ```Highly Rated``` and ```Popular```  restaurants.\n",
    "\n",
    "\n",
    "   \n",
    "### Setup\n",
    "1. To install the application we have used Pipenv\n",
    "2. To install dependencies run `pipenv install`\n",
    "3. To view the Notebook run `pipenv run jupyter notebook`\n",
    "4. Once executed one can view and run the unpopular.ipynb notebook\n",
    "\n",
    "### Libraries used\n",
    "1. pandas - for dataframe manipulation\n",
    "2. nltk - for lemmatization and stop words removal\n",
    "3. gensim - for word2vec modelling\n",
    "4. numpy - for Dataframe processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import multiprocessing as mp\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch dataset\n",
    "1. Create dataset for business's and associated reviews\n",
    "2. Final dataset has Reviews for all restaurant's\n",
    "3. Filter out only US states.\n",
    "4. Convert Categorical Reviews by using the following \n",
    "```\n",
    "Review >=3.5 Then 1\n",
    "Else -1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "business_ds = pd.read_json('yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json',lines=True)\n",
    "all_review_ds = pd.read_json('yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json',lines=True).sample(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "categories=['Turkish','Indian','Mexican','Italian','Chinese', \\\n",
    "            'Mediterranean','American (New)','American (Traditional)', \\\n",
    "            'Lebanese','Moroccan','Afghan', \\\n",
    "           'Bangladeshi','Japanese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_business_ds=business_ds.loc[business_ds['categories'].apply(lambda x:'Restaurants' in list(x) )] \\\n",
    ".loc[business_ds['state'].apply(lambda x:x in states)]\n",
    "all_business_reviews = restaurant_business_ds.set_index('business_id').join(all_review_ds.set_index('business_id'),how='inner',lsuffix='_bus', rsuffix='_rev') \\\n",
    "[['name','text','categories','state','review_count','stars_bus','stars_rev','full_address','longitude','latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_business_reviews['rest_rating'] = all_business_reviews.apply(lambda row :1 if row['stars_bus']>=3.5 else -1 , axis = 1)\n",
    "all_business_reviews['rev_rating'] = all_business_reviews.apply(lambda row :1 if row['stars_rev']>=3.5 else -1 , axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset cleanup\n",
    "1. Use nltk stopwords to eliminate all stop words\n",
    "2. Lemmatize the words again using nltk library\n",
    "3. Using gensim simple_process remove punctuations and unnecessary characters\n",
    "4. Split sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zztop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zztop/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# stop_words = stopwords.words('english')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocessor(sentence):\n",
    "    return [lemmatizer.lemmatize(word) for word in simple_preprocess(str(sentence),deacc=True) if word not in STOP_WORDS]\n",
    "\n",
    "# def preprocess_sentenses(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(preprocessor(sentence))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reviews_all_categories(revs):\n",
    "    for cat in revs['categories']:\n",
    "        if cat not in categories:\n",
    "            continue\n",
    "        return {'categories':cat,'text':revs['text'],'name':revs['name'],'state':revs['state'], \\\n",
    "                'rest_rating':revs['rest_rating'],'rev_rating':revs['rev_rating'],'review_count':revs['review_count'] \\\n",
    "               ,'full_address':revs['full_address'],'longitude':revs['longitude'],'latitude':revs['latitude']}\n",
    "reviews=[]\n",
    "   \n",
    "# business_reviews.iloc[:1]['categories']\n",
    "with mp.Pool(processes = 6) as p:\n",
    "    reviews[:] = p.map(get_reviews_all_categories, (revs for _, revs in all_business_reviews.iterrows()))\n",
    "reviews = pd.DataFrame(filter(lambda x: x is not None, reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"words\"]=reviews.apply(lambda rev: preprocessor(rev['text']),axis=1)\n",
    "reviews=reviews.drop([\"text\"], axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trigram models\n",
    "1. We used gensim Phraser library to create trigram models using word collocation\n",
    "2. Parameter used are\n",
    "    1. min_count=3 - Atleast the word appear three times\n",
    "    2. Behind the scene it uses (Normalized Pointwise Mutual Information(NPMI)) to score for forming phrases.\n",
    "    3. A phrase with word a followed by word b is accepted if the NPMI score is above a threshold\n",
    "    4. We have left the threshold to be default value of 7. \n",
    "3. Sum up all the trigram reviews per category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "bigram = Phrases(reviews['words'].values, min_count=3, threshold=7)\n",
    "trigram = Phrases(bigram[reviews['words'].values], min_count=3,threshold=7)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "data_words_trigrams = make_trigrams(reviews['words'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['trigram'] =data_words_trigrams\n",
    "reviews=reviews.drop([\"words\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# reviews_dask_df = dd.from_pandas(reviews, npartitions=6)\n",
    "\n",
    "# categories_all_reviews_dask_ds=reviews_dask_df.groupby([\"categories\"]).agg({'trigram': sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_all_reviews_dask_ds = categories_all_reviews_dask_ds.reset_index()\n",
    "# categories_all_reviews_dask_ds=categories_all_reviews_dask_ds.set_index(categories_all_reviews_dask_ds['categories'])\n",
    "# categories_all_reviews_dask_ds=categories_all_reviews_dask_ds.drop([\"categories\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_all_reviews_ds=categories_all_reviews_dask_ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_all_reviews_ds =reviews.groupby([\"categories\"]).agg({'trigram': sum})\n",
    "# categories_all_reviews_ds = categories_all_reviews_ds.reset_index()\n",
    "# categories_all_reviews_ds.set_index(categories_all_reviews_ds['categories'],inplace=True)\n",
    "# categories_all_reviews_ds=categories_all_reviews_ds.drop([\"categories\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_all_reviews_ds = categories_all_reviews_ds.reset_index()\n",
    "categories_all_reviews_ds=categories_all_reviews_ds.set_index(categories_all_reviews_ds['categories'])\n",
    "categories_all_reviews_ds=categories_all_reviews_ds.drop([\"categories\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorized word model using Word2Vec\n",
    "1. Both the models uses Neural Network that uses the context of the word for creating similar representation with similar meaning words\n",
    "2. Traditional way's represents word vectors as one hot rod, that is a vector for only one word/phrase.\n",
    "    1. The length of the vector is equal to the size of the total unique vocabulary in the corpora.\n",
    "    2. However with this representation its not easy to infer relationship between two given words.\n",
    "    3. Hence word with similar meaning are treated as two separate vectors in one hot rod vector representation\n",
    "3. Word2Vec uses context of the word, essentially words in its neighborhood to create word representation.\n",
    "    1. It uses a Neural Network whose hidden layer essentially encodes the word representation\n",
    "    2. Its of two type,Skip-gram and CBOW. The difference is in former the input to the Neural Net is the targetted word and output is its surrounding word, whereas in latter the input and output are swapped.\n",
    "    3. Essentially the main difference is how the word vectors are created.\n",
    "    4. Generally the performances of both models are similar and I will go with Skip-Gram, which supposedly does a better job with rare words\n",
    "4. Fasttext is an extension to Word2Vec. It treats each word as set of ngrams as it feeds it to the Neural Net.\n",
    "    1. It is generally better than Word2Vec as breaking a rare word into ngrams increases the chances of finding more neighborhood context words.\n",
    "    2. Its also efficient with Out of vocabulory words as creating a ngram representation may help find hidden neighborhood context words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(data_words_trigrams, min_count=10,seed=1,sg=1)\n",
    "# fasttext_model = FastText(categories_all_reviews_ds.trigrams, min_count=10,seed=1,sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Dissimilarities between Cuisine Categories\n",
    "1. Using the Word2Vec model from above we aggregate all word vectors using mean pooling\n",
    "    1. By Mean pooling we convert a multi set Word2Vec model into a single high dimensional vector with a constant length. More can be read [here](https://www.cs.tau.ac.il/~wolf/papers/qagg.pdf)\n",
    "2. Using the mean pooled word2vec dataframe we find correlatation between all the categories.\n",
    "3. We used Pearson's r to calculate correlation as suggested [here](https://www.aclweb.org/anthology/D19-1008.pdf)\n",
    "4. From the output Correlation matrix, given a Category we can calculate the least correlated Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from Word Vectors - https://www.kaggle.com/sakshat/word2vec-xgboost\n",
    "import numpy as np\n",
    "def makeFeatureVec(words,model,num_features):\n",
    "    feature_vec=np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords=0\n",
    "    index2word_set=set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords=nwords+1\n",
    "            feature_vec=np.add(feature_vec,model[word])\n",
    "    feature_vec=np.divide(feature_vec,nwords)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_mean_df = pd.DataFrame([makeFeatureVec(words,word2vec_model,word2vec_model.wv.vector_size)  for words in categories_all_reviews_ds.trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_mean_df.set_index(categories_all_reviews_ds.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_mean_corr_df = word2vec_mean_df.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_mean_corr_df['Least_Correlated']=word2vec_mean_corr_df \\\n",
    ".apply(lambda cuisine:cuisine.idxmin() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "Afghan                                  Japanese\n",
       "American (New)                            Indian\n",
       "American (Traditional)                  Moroccan\n",
       "Chinese                                 Moroccan\n",
       "Indian                    American (Traditional)\n",
       "Italian                                   Indian\n",
       "Japanese                                Moroccan\n",
       "Lebanese                                Japanese\n",
       "Mediterranean                           Japanese\n",
       "Mexican                                 Moroccan\n",
       "Moroccan                                 Mexican\n",
       "Turkish                                 Japanese\n",
       "Name: Least_Correlated, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_mean_corr_df[['Least_Correlated']].to_json('Least_Correlated.json')\n",
    "word2vec_mean_corr_df['Least_Correlated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_mean_corr_df.to_json('popularity.json',orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Restaurants\n",
    "We Rate a restaurant in two ways\n",
    "    1. Popularity \n",
    "    2. Highly Rated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity\n",
    "1. To find the most Popular restaurants we get the restaurants with most number of reviews per Category per State\n",
    "2. Save the results in a json object to be consumed by the Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>full_address</th>\n",
       "      <th>review_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>China Magic Noodle House</td>\n",
       "      <td>2015 N Dobson Rd\\nChandler, AZ 85224</td>\n",
       "      <td>240</td>\n",
       "      <td>33.336409</td>\n",
       "      <td>-111.876379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>China Chili</td>\n",
       "      <td>302 E Flower St\\nPhoenix, AZ 85012</td>\n",
       "      <td>224</td>\n",
       "      <td>33.485850</td>\n",
       "      <td>-112.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Snoh Ice Shavery</td>\n",
       "      <td>914 E Camelback Rd\\nUnit 4B\\nPhoenix, AZ 85014</td>\n",
       "      <td>222</td>\n",
       "      <td>33.509529</td>\n",
       "      <td>-112.060958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categories state                      name  \\\n",
       "26    Chinese    AZ  China Magic Noodle House   \n",
       "27    Chinese    AZ               China Chili   \n",
       "28    Chinese    AZ          Snoh Ice Shavery   \n",
       "\n",
       "                                      full_address  review_count   latitude  \\\n",
       "26            2015 N Dobson Rd\\nChandler, AZ 85224           240  33.336409   \n",
       "27              302 E Flower St\\nPhoenix, AZ 85012           224  33.485850   \n",
       "28  914 E Camelback Rd\\nUnit 4B\\nPhoenix, AZ 85014           222  33.509529   \n",
       "\n",
       "     longitude  \n",
       "26 -111.876379  \n",
       "27 -112.069181  \n",
       "28 -112.060958  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_ds =reviews[['categories','name','state','full_address','review_count','latitude','longitude']].drop_duplicates() \\\n",
    ".sort_values(['categories','review_count'],ascending=False) \\\n",
    ".groupby(['categories','state']).nth((0,1,2)).reset_index() \\\n",
    "\n",
    "popularity_ds.to_json('popularity.json',orient='records')\n",
    "\n",
    "popularity_ds.loc[popularity_ds['categories']=='Chinese'].loc[popularity_ds['state']=='AZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highly Rated\n",
    "1. To find the most Highly Rated restaurant we calculate user sentiments by calculating the percent of positive ratings, rather than taken average user rating.\n",
    "```\n",
    "User Sentiment of a restaurant = (Sum Of All Positive Reviews)/(Total Number of Reviews)\n",
    "```\n",
    "2. Using the sentiments, find the restaurant with the Highest Sentiment per Category, per State\n",
    "2. Save the results in a json object to be consumed by the Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>state</th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>full_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2595</td>\n",
       "      <td>Jade Palace</td>\n",
       "      <td>8120 N Hayden Rd\\nScottsdale, AZ 85258</td>\n",
       "      <td>33.555735</td>\n",
       "      <td>-111.898815</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2749</td>\n",
       "      <td>Panda Express</td>\n",
       "      <td>1747 E Florence Blvd\\nCasa Grande, AZ 85122</td>\n",
       "      <td>32.879219</td>\n",
       "      <td>-111.712176</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2967</td>\n",
       "      <td>Wongs To Go</td>\n",
       "      <td>5219 S 7th St\\nPhoenix, AZ 85040</td>\n",
       "      <td>33.398942</td>\n",
       "      <td>-112.064389</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categories state  index           name  \\\n",
       "65    Chinese    AZ   2595    Jade Palace   \n",
       "66    Chinese    AZ   2749  Panda Express   \n",
       "67    Chinese    AZ   2967    Wongs To Go   \n",
       "\n",
       "                                   full_address   latitude   longitude  \\\n",
       "65       8120 N Hayden Rd\\nScottsdale, AZ 85258  33.555735 -111.898815   \n",
       "66  1747 E Florence Blvd\\nCasa Grande, AZ 85122  32.879219 -111.712176   \n",
       "67             5219 S 7th St\\nPhoenix, AZ 85040  33.398942 -112.064389   \n",
       "\n",
       "    sentiment  \n",
       "65   0.714286  \n",
       "66   0.666667  \n",
       "67   0.666667  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_count_df =reviews[['categories','name','state','full_address','rev_rating','review_count','latitude','longitude']].loc[reviews['rev_rating']==1] \\\n",
    ".groupby(['categories','name','full_address','latitude','longitude','state','review_count']) \\\n",
    ".agg( postive_review_count=pd.NamedAgg(column='rev_rating', aggfunc='count')) \\\n",
    ".reset_index() \n",
    "\n",
    "review_count_df['sentiment']=review_count_df['postive_review_count']/review_count_df['review_count']\n",
    "\n",
    "sentiment_df = review_count_df.sort_values(['categories','sentiment'],ascending=False).reset_index() \\\n",
    ".groupby(['categories','state']).nth((0,1,2)).sort_values(['categories','state','sentiment'],ascending=False).reset_index()\n",
    "sentiment_df=sentiment_df.drop(columns=['review_count','postive_review_count'])\n",
    "sentiment_df.to_json('sentiment.json',orient='records')\n",
    "\n",
    "sentiment_df.loc[sentiment_df['categories']=='Chinese'].loc[sentiment_df['state']=='AZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NV', 'AZ', 'WI', 'GA'], dtype=object),\n",
       " array(['AZ', 'NV', 'WI', 'GA'], dtype=object))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.state.unique(),popularity_ds.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. From the above scatter plot we can see clusters of words/phrases representing several indian cuisines.\n",
    "2. These word associations can used by users to explore more different Indian dishes, for eg\n",
    "    1. If you like tandoori chicken, you will also like to try Butter chicken\n",
    "3. Using just one desert as input we got several other deserts. Eg\n",
    "    1. with rice_pudding we discovered, mango ice cream, kheer, mango pudding.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "1. For one there was no model evaluation. We did not score models based on input labels\n",
    "2. Use Segphrase to find input labels and feed into Fastext to find similar words\n",
    "3. Evaluate accuracy of the generated model.\n",
    "4. We used top 5 most similar phrases. However we should put a threshold on similarity and get all labels above this threshold. This will weed out weak labels, and possibly get a more broader feature base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
